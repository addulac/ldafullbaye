%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On perplexity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Perplexity is a measure based on the posterior probability of observing a new occurrence of a word $w$ in a document $d$, which is given by
\begin{equation}
\label{eqn:perplex}
\tilde{p}(w|d) =
\sum_k\tilde{p}(w,k|d) =
\sum_k\expectation_{\vmDK_{DK},\vmKW_{KW}\sim\tilde{p}}[\vmDK_{dk}\vmKW_{kw}]
\end{equation}
If we replace the model posterior $\tilde{p}$ by its VB approximation in the expectation, one obtains a product of two marginal expectations since the variables $\vmDK_{dk}\vmKW_{kw}$ are assumed independent in the VB approximation. The marginal expectations are then easy to compute. However, we follow instead a different method, empirically suggested in~\cite{asuncion_smoothing_2009}, and which we justify here. The model posterior expectation in~(\ref{eqn:perplex}) is an arithmetic mean, which is lower bounded by its corresponding geometric mean. The latter factorises in $\vmDK_{dk}\vmKW_{kw}$, even if they are not independent (and they are not, in the model posterior). Now the geometric mean of $\vmDK_{dk}$ (resp. $\vmKW_{kw}$) can be computed in the VB approximation, by simple application of the definition of $\vvmbDK_{dk},\vvmbKW_{kw}$, resulting in
\begin{equation*}
\tilde{p}(w|d) \gtrapprox \sum_k\exp-(\vvmbDK_{dk}+\vvmbKW_{kw})
\end{equation*}
This approximate lower bound, which is also the normaliser in update~$\ruleref{D}$, is used in the definition of the perplexity, where it becomes an upper bound. Up to notations, it is the same term as in Equation (16) of~\cite{asuncion_smoothing_2009}, but it is unclear whether this is the same term as in Equation (16) of~\cite{hoffman_online_2010}.