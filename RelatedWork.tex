
The LDA model \cite{blei_latent_2003} has been extended in many different ways to address different problems. Information about document authorship has, for example, been added to the model in \cite{Rosen_Zvi_2004}, whereas the integration of correlations between topics in the model has been explored in \cite{blei_correlated_2007}; more recently, \cite{Boyd_Graber_2009} describes an extension to model multilingual unaligned collections. Other extensions have focused on streaming or online versions of the model: \cite{Yao_2009} focuses on efficient methods for inference in streaming collections, whereas \cite{Wang_2012} introduces a new model for text streams based on transition probabilities between topics of successive documents and \cite{hoffman_online_2010} proposes an online variational Bayes algorithm for LDA based on mini-batches.

Inference in LDA is usually performed through variational Bayes (as proposed in the original LDA paper \cite{blei_latent_2003}) or Gibbs sampling (as proposed in \cite{griffiths04finding}). Both methods have been extensively studied and collapsed versions, resulting in faster inference, have been proposed \cite{teh_collapsed_2006,porteous_fast_2008}. This last study, besides comparing the different approaches to inference in LDA, introduces gamma priors on the hyperparameters\footnote{A similar prior is used in \cite{Gorur_2006} for Indian Buffet Processes.} which yield more stable models (in the sense that the difference between inference methods disappears when the priors are used). \cite{wallach_rethinking_2009} goes one step further by assessing the importance of priors on both the document-topic and topic-word distributions. The priors considered are asymmetric Dirichlet priors and the authors show that the use of such asymmetric priors has substantial advantages over the use of symmetric priors, especially for the document-topic distribution.

We follow here a similar approach but consider a complete family of priors, namely the conjugate prior to the Dirichlet distribution, which we introduce in the next section. As for asymmetric Dirichlet priors, the use of a conjugate prior leads to efficient inference methods; it also yields a broader family of distributions that provides a complete Bayesian treatment (and re-interpretation) of the variational Bayes inference in the LDA model.
